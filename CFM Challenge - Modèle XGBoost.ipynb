{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"/Users/julesmourgues/Documents/Programmation/CFM/\"\n",
    "\n",
    "#Load x and y train data\n",
    "input_test = pd.read_csv(path + \"input_test.csv\")\n",
    "input_training = pd.read_csv(path + \"input_training.csv\")\n",
    "#output_test_random = pd.read_csv(path + \"output_test_random.csv\")\n",
    "output_training = pd.read_csv(path + \"output_training_gmEd6Zt.csv\")\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def create_and_assign_clusters(df, num_clusters, column):\n",
    "    # Sort by equity and day\n",
    "    if column == \"returns\":\n",
    "        df['func'] = df.iloc[:, 3:3+53].sum(axis = 1) # Sum of the 52 first columns\n",
    "    elif column == \"volatility\":\n",
    "        df['func'] = df.iloc[:, 3:3+53].std(axis = 1)\n",
    "\n",
    "    new_df = df.pivot_table(index='day', columns='equity', values='func')\n",
    "    new_df.columns.name = None\n",
    "    func = new_df.fillna(0)\n",
    "\n",
    "    # Calculate the Pearson correlation matrix and convert it to distances\n",
    "    distance_matrix = np.sqrt(0.5 * (1 - func.corr(method='pearson')))\n",
    "\n",
    "    # Perform hierarchical clustering using Ward's method\n",
    "    linked = linkage(np.nan_to_num(squareform(distance_matrix, checks=False), nan=0, posinf=1e9, neginf=-1e9), method='ward')\n",
    "\n",
    "    # Use the fcluster function to assign cluster labels to tickers\n",
    "    cluster_labels = fcluster(linked, num_clusters, criterion='maxclust')\n",
    "\n",
    "    # Create a DataFrame with the cluster labels and corresponding tickers\n",
    "    clustered_data = pd.DataFrame({'Ticker': func.columns, 'Cluster': cluster_labels})\n",
    "\n",
    "    # Use a loop to extract each cluster and store it in a separate DataFrame\n",
    "    clusters = {i: clustered_data[clustered_data['Cluster'] == i]['Ticker'] for i in range(1, num_clusters + 1)}\n",
    "\n",
    "    if df['equity'].max() >= 1000000:\n",
    "        for cluster in clusters:\n",
    "            for index in clusters[cluster].index:\n",
    "                if column == \"returns\":\n",
    "                    df.loc[df['equity'] == (1000000 + index), 'cluster_returns'] = cluster\n",
    "                elif column == \"volatility\":\n",
    "                    df.loc[df['equity'] == (1000000 + index), 'cluster_volatility'] = cluster\n",
    "    else:\n",
    "        for cluster in clusters:\n",
    "            for index in clusters[cluster].index:\n",
    "                if column == \"returns\":\n",
    "                    df.loc[df['equity'] == index, 'cluster_returns'] = cluster\n",
    "                elif column == \"volatility\":\n",
    "                    df.loc[df['equity'] == index, 'cluster_volatility'] = cluster\n",
    "\n",
    "    #Suoprimer la colonne returns\n",
    "    df = df.drop(columns = ['func'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(input_training, input_test, outlier=0.05, groupby=False, IQRoutlier = True, variables = False, na_count = False, clusters = False, cluster_type = \"returns\", suppr = False, nbforlast = 5):\n",
    "    import pandas as pd\n",
    "    from datetime import time, timedelta, datetime, date\n",
    "\n",
    "    df_train = input_training.copy()\n",
    "    #Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "    if na_count is not False:\n",
    "        df_train['na_count'] = df_train.isnull().sum(axis=1)\n",
    "      \n",
    "        #Remplace les inf par des 60\n",
    "        df_train = df_train.replace([np.inf, -np.inf], 60)\n",
    "    df_train = df_train.fillna(0)\n",
    "    df_train = df_train.sort_values(by=['equity', 'day'])\n",
    "\n",
    "    df_test = input_test.copy()\n",
    "    #Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "    if na_count is not False:\n",
    "        df_test['na_count'] = df_test.isnull().sum(axis=1)\n",
    "\n",
    "        #Remplace les inf par des 60\n",
    "        df_test = df_test.replace([np.inf, -np.inf], 60)\n",
    "    df_test = df_test.fillna(0)\n",
    "    df_test = df_test.sort_values(by=['equity', 'day'])\n",
    "\n",
    "    if outlier is not False:\n",
    "        for col in df_train.iloc[:,3:3+53].columns:\n",
    "            Q1 = df_train[col].quantile(outlier)\n",
    "            Q3 = df_train[col].quantile(1 - outlier)\n",
    "            IQR = Q3 - Q1\n",
    "            median = df_train[col].median()\n",
    "            if IQRoutlier is not False:\n",
    "                df_train.loc[(df_train[col] < (Q1 - 1.5 * IQR)), col] = Q1 - 1.5 * IQR\n",
    "                df_train.loc[(df_train[col] > (Q3 + 1.5 * IQR)), col] = Q3 + 1.5 * IQR\n",
    "            else:\n",
    "                df_train.loc[((df_train[col] < ((Q1 - 1.5 * IQR))) | (df_train[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "        for col in df_test.iloc[:,3:3+53].columns:\n",
    "            Q1 = df_test[col].quantile(outlier)\n",
    "            Q3 = df_test[col].quantile(1 - outlier)\n",
    "            IQR = Q3 - Q1\n",
    "            median = df_test[col].median()\n",
    "            if IQRoutlier is not False:\n",
    "                df_test.loc[(df_test[col] < (Q1 - 1.5 * IQR)), col] = Q1 - 1.5 * IQR\n",
    "                df_test.loc[(df_test[col] > (Q3 + 1.5 * IQR)), col] = Q3 + 1.5 * IQR\n",
    "            else:\n",
    "                df_test.loc[((df_test[col] < ((Q1 - 1.5 * IQR))) | (df_test[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "\n",
    "    if clusters is not False:\n",
    "        if cluster_type == \"returns\":\n",
    "            df_train = create_and_assign_clusters(df_train, clusters, \"returns\")\n",
    "            df_test = create_and_assign_clusters(df_test, clusters, \"returns\")\n",
    "        elif cluster_type == \"volatility\":\n",
    "            df_train = create_and_assign_clusters(df_train, clusters, \"volatility\")\n",
    "            df_test = create_and_assign_clusters(df_test, clusters, \"volatility\")\n",
    "\n",
    "    def volatility(df):\n",
    "            df['volatility'] = df.iloc[:,3:3+53].std(axis=1)\n",
    "            df['returns'] = df.iloc[:,3:3+53].sum(axis=1)\n",
    "            df['last_returns'] = df.iloc[:,3+53-nbforlast:3+53].iloc[:,-1]\n",
    "\n",
    "    if variables == \"volatility\":\n",
    "        volatility(df_train)\n",
    "        volatility(df_test)\n",
    "    elif variables == \"market+volatility\":\n",
    "        volatility(df_train)\n",
    "        volatility(df_test)\n",
    "\n",
    "        def calculate_market_means1(df, group_col, value_cols, beta = True):\n",
    "\n",
    "            grouped_values = df.groupby(group_col)[value_cols]\n",
    "\n",
    "            mean_col = grouped_values.mean()\n",
    "            #rolling_mean_col = mean_col.rolling(5).mean()\n",
    "\n",
    "            df[f'market_mean_{value_cols}_{group_col}'] = df[group_col].map(mean_col)\n",
    "            if beta == True:\n",
    "                #df[f'rolling_market_mean_{value_col}_{group_col}'] = df[group_col].map(rolling_mean_col)\n",
    "                df[f'{group_col}_{value_cols}_beta'] = df[value_cols] / df[f'market_mean_{value_cols}_{group_col}']\n",
    "\n",
    "            '''if IQRindicateur is True:\n",
    "                if value_col == 'volatility':\n",
    "                    Q1 = grouped_values.quantile(0.25)\n",
    "                    Q3 = grouped_values.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "\n",
    "                    df[f'{value_col}_Q1_{group_col}'] = df[group_col].map(Q1)\n",
    "                    df[f'{value_col}_Q3_{group_col}'] = df[group_col].map(Q3)\n",
    "                    df[f'{value_col}_IQR_{group_col}'] = df[group_col].map(IQR)'''\n",
    "\n",
    "        def calculate_market_means(df, group_cols, value_cols, beta = True):\n",
    "            grouped_values = df.groupby(group_cols)[value_cols]\n",
    "\n",
    "            mean_col = grouped_values.mean()\n",
    "\n",
    "            # Create a multi-indexed series with the mean values\n",
    "            mean_series = pd.Series(mean_col.values, index=pd.MultiIndex.from_tuples(mean_col.index))\n",
    "\n",
    "            # Map the multi-indexed series to the original dataframe\n",
    "            df[f'market_mean_{value_cols}_{\"_\".join(group_cols)}'] = df.set_index(group_cols).index.map(mean_series)\n",
    "\n",
    "            if beta == True:\n",
    "                df[f'{\"_\".join(group_cols)}_{value_cols}_beta'] = df[value_cols] / df[f'market_mean_{value_cols}_{\"_\".join(group_cols)}']\n",
    "\n",
    "\n",
    "\n",
    "        # Utiliser la fonction calculate_market_means pour les colonnes 'returns' et 'volatility'\n",
    "        calculate_market_means1(df_train, 'day', 'volatility') \n",
    "        calculate_market_means1(df_test, 'day', 'volatility')\n",
    "        calculate_market_means1(df_train, 'equity', 'volatility') \n",
    "        calculate_market_means1(df_test, 'equity', 'volatility')\n",
    "        #calculate_market_means1(df_train, 'day', 'returns')\n",
    "        #calculate_market_means1(df_test, 'day', 'returns')\n",
    "        calculate_market_means1(df_train, 'day', 'last_returns')\n",
    "        calculate_market_means1(df_test, 'day', 'last_returns')\n",
    "\n",
    "        if cluster_type == \"returns\" and clusters is not False:\n",
    "            calculate_market_means(df_train, ['cluster_returns', 'day'], 'volatility', beta = True)\n",
    "            calculate_market_means(df_test, ['cluster_returns', 'day'], 'volatility', beta = True)\n",
    "        elif cluster_type == \"volatility\" and clusters is not False:\n",
    "            calculate_market_means(df_train, ['cluster_volatility', 'day'], 'volatility', beta = True)\n",
    "            calculate_market_means(df_test, ['cluster_volatility', 'day'], 'volatility', beta = True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        #calculate_market_means(df_test, 'day', 'volatility') \n",
    "    #calculate_market_means(df, 'equity', ['returns', 'volatility', 'last_returns', 'last_volatility']) \n",
    "\n",
    "    if groupby is False:\n",
    "        #df_train['next_day_r0'] = df_train.groupby('equity')['r0'].shift(-1)\n",
    "        #df_test['next_day_r0'] = df_test.groupby('equity')['r0'].shift(-1)\n",
    "        print(\"No groupby\")\n",
    "    else:\n",
    "        firstrtime = 35\n",
    "        new_labels = []\n",
    "        start_time = time(9, firstrtime)\n",
    "        for i in range(53):\n",
    "            new_labels.append(start_time.strftime('%H:%M'))\n",
    "            start_time = (datetime.combine(date.today(), start_time) + timedelta(minutes=5)).time()\n",
    "\n",
    "        col_rename_dict = {f'r{i}': new_labels[i] for i in range(53)}\n",
    "        df_train.rename(columns=col_rename_dict, inplace=True)\n",
    "        df_test.rename(columns=col_rename_dict, inplace=True)\n",
    "\n",
    "        id_column_train = df_train['ID']\n",
    "        id_column_test = df_test['ID']\n",
    "        interval_dict = {label: \"{:.2f}\".format(datetime.strptime(label, '%H:%M').hour + (datetime.strptime(label, '%H:%M').minute // groupby) / (60/groupby)) for label in new_labels}\n",
    "        \n",
    "        df_train_grouped_interval = df_train.groupby(interval_dict, axis=1).sum()\n",
    "        df_train_grouped_interval['ID'] = id_column_train\n",
    "        df_train = df_train.merge(df_train_grouped_interval, on = 'ID', how='left')\n",
    "        df_train = df_train.drop(df_train.columns[3:3+53], axis = 1)\n",
    "        #df_train['next_day_first_interval'] = df_train.groupby('equity')[list(interval_dict.values())[1]].shift(-1)\n",
    "\n",
    "        df_test_grouped_interval = df_test.groupby(interval_dict, axis=1).sum()\n",
    "        df_test_grouped_interval['ID'] = id_column_test\n",
    "        df_test = df_test.merge(df_test_grouped_interval, on = 'ID', how='left')\n",
    "        df_test = df_test.drop(df_test.columns[3:3+53], axis = 1)\n",
    "        #df_test['next_day_first_interval'] = df_test.groupby('equity')[list(interval_dict.values())[1]].shift(-1)\n",
    "\n",
    "    if suppr is True:\n",
    "        df_train = df_train.drop(df_train.columns[3:3+53], axis = 1)\n",
    "        df_test = df_test.drop(df_test.columns[3:3+53], axis = 1)\n",
    "\n",
    "    df_train = df_train.merge(output_training, on = 'ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_count = False\n",
    "outlier = 0.05\n",
    "\n",
    "df_train = input_training.copy()\n",
    "#Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "if na_count is not False:\n",
    "    df_train['na_count'] = df_train.isnull().sum(axis=1)\n",
    "df_train = df_train.fillna(0)\n",
    "df_train = df_train.sort_values(by=['equity', 'day'])\n",
    "\n",
    "df_test = input_test.copy()\n",
    "#Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "if na_count is not False:\n",
    "    df_test['na_count'] = df_test.isnull().sum(axis=1)\n",
    "df_test = df_test.fillna(0)\n",
    "df_test = df_test.sort_values(by=['equity', 'day'])\n",
    "\n",
    "for col in df_train.iloc[:,3:3+53].columns:\n",
    "    Q1 = df_train[col].quantile(outlier)\n",
    "    Q3 = df_train[col].quantile(1 - outlier)\n",
    "    IQR = Q3 - Q1\n",
    "    median = df_train[col].median()\n",
    "    df_train.loc[((df_train[col] < ((Q1 - 1.5 * IQR))) | (df_train[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "for col in df_test.iloc[:,3:3+53].columns:\n",
    "    Q1 = df_test[col].quantile(outlier)\n",
    "    Q3 = df_test[col].quantile(1 - outlier)\n",
    "    IQR = Q3 - Q1\n",
    "    median = df_test[col].median()\n",
    "    df_test.loc[((df_test[col] < ((Q1 - 1.5 * IQR))) | (df_test[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "\n",
    "df_train_cumsum = df_train.copy()\n",
    "df_train_cumsum.insert(3, 'rinitiale', 0)\n",
    "df_train_cumsum.iloc[:,4:4+53] = df_train_cumsum.iloc[:,4:4+53].cumsum(axis=1)\n",
    "#df_train_cumsum.iloc[:,3:4+53] = df_train_cumsum.iloc[:,3:4+53] +1000000\n",
    "\n",
    "\n",
    "df_test_cumsum = df_test.copy()\n",
    "df_test_cumsum.insert(3, 'rinitiale', 0)\n",
    "df_test_cumsum.iloc[:,4:4+53] = df_test_cumsum.iloc[:,4:4+53].cumsum(axis=1)\n",
    "#df_test_cumsum.iloc[:,3:4+53] = df_test_cumsum.iloc[:,3:4+53] +1000000\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Group by day and calculate mean cumsum\n",
    "df_train_grouped = df_train_cumsum.iloc[:,3:4+53].groupby(df_train_cumsum['day']).mean()\n",
    "\n",
    "# Calculate area under the curve for each group\n",
    "df_train_grouped['auc'] = df_train_grouped.apply(lambda row: np.trapz(row), axis=1)\n",
    "\n",
    "# Subtract area under the curve of each row from the group's auc\n",
    "df_train_cumsum = df_train_cumsum.merge(df_train_grouped['auc'], on='day', how='left')\n",
    "df_train_cumsum['diff'] = df_train_cumsum.iloc[:,3:4+53].apply(lambda row: np.trapz(row), axis=1) / df_train_cumsum['auc'] \n",
    "df_train_cumsum['r52_mean_day'] = df_train_cumsum.groupby('day')['r52'].transform('mean')\n",
    "df_train_cumsum['r52_mean_equity'] = df_train_cumsum.groupby('equity')['r52'].transform('mean')\n",
    "\n",
    "# Group by day and calculate mean cumsum\n",
    "df_test_grouped = df_test_cumsum.iloc[:,3:4+53].groupby(df_test_cumsum['day']).mean()\n",
    "\n",
    "# Calculate area under the curve for each group\n",
    "df_test_grouped['auc'] = df_test_grouped.apply(lambda row: np.trapz(row), axis=1)\n",
    "\n",
    "# Subtract area under the curve of each row from the group's auc\n",
    "df_test_cumsum = df_test_cumsum.merge(df_test_grouped['auc'], on='day', how='left')\n",
    "df_test_cumsum['diff'] = df_test_cumsum.iloc[:,3:4+53].apply(lambda row: np.trapz(row), axis=1)/df_test_cumsum['auc']\n",
    "df_test_cumsum['r52_mean_day'] = df_test_cumsum.groupby('day')['r52'].transform('mean')\n",
    "df_test_cumsum['r52_mean_equity'] = df_test_cumsum.groupby('equity')['r52'].transform('mean')\n",
    "\n",
    "df_test_cumsum['r52_mean'] = df_test_cumsum.groupby('equity')['r52'].transform('mean')\n",
    "\n",
    "\n",
    "\n",
    "# Rename 'auc' to 'auc_day'\n",
    "df_train_cumsum.rename(columns={'auc': 'auc_day'}, inplace=True)\n",
    "df_test_cumsum.rename(columns={'auc': 'auc_day'}, inplace=True)\n",
    "\n",
    "# Group by equity and calculate mean cumsum\n",
    "df_train_grouped_equity = df_train_cumsum.iloc[:,3:4+53].groupby(df_train_cumsum['equity']).mean()\n",
    "df_test_grouped_equity = df_test_cumsum.iloc[:,3:4+53].groupby(df_test_cumsum['equity']).mean()\n",
    "\n",
    "# Calculate area under the curve for each group\n",
    "df_train_grouped_equity['auc_equity'] = df_train_grouped_equity.apply(lambda row: np.trapz(row), axis=1)\n",
    "df_test_grouped_equity['auc_equity'] = df_test_grouped_equity.apply(lambda row: np.trapz(row), axis=1)\n",
    "\n",
    "# Subtract area under the curve of each row from the group's auc_equity\n",
    "df_train_cumsum = df_train_cumsum.merge(df_train_grouped_equity['auc_equity'], on='equity', how='left')\n",
    "df_train_cumsum['diff_equity'] = df_train_cumsum.iloc[:,3:4+53].apply(lambda row: np.trapz(row), axis=1) / df_train_cumsum['auc_equity'] \n",
    "\n",
    "df_test_cumsum = df_test_cumsum.merge(df_test_grouped_equity['auc_equity'], on='equity', how='left')\n",
    "df_test_cumsum['diff_equity'] = df_test_cumsum.iloc[:,3:4+53].apply(lambda row: np.trapz(row), axis=1) / df_test_cumsum['auc_equity'] \n",
    "df_test_cumsum.set_index('ID', inplace=True)\n",
    "df_train_cumsum.set_index('ID', inplace=True)\n",
    "\n",
    "#df_train = df_train.merge(df_train_cumsum[['auc', 'diff']], on = 'ID', how='left')\n",
    "#df_test = df_test.merge(df_test_cumsum[['auc', 'diff']], on = 'ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>rinitiale</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>auc_day</th>\n",
       "      <th>diff</th>\n",
       "      <th>r52_mean_day</th>\n",
       "      <th>r52_mean_equity</th>\n",
       "      <th>auc_equity</th>\n",
       "      <th>diff_equity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>832716</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.61</td>\n",
       "      <td>-10.09</td>\n",
       "      <td>12.57</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>-5.02</td>\n",
       "      <td>...</td>\n",
       "      <td>138.81</td>\n",
       "      <td>136.33</td>\n",
       "      <td>156.20</td>\n",
       "      <td>158.68</td>\n",
       "      <td>1075.783995</td>\n",
       "      <td>2.147903</td>\n",
       "      <td>49.866790</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>-37.829751</td>\n",
       "      <td>-61.081025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403174</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.83</td>\n",
       "      <td>-44.50</td>\n",
       "      <td>-26.97</td>\n",
       "      <td>-2.07</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.38</td>\n",
       "      <td>32.65</td>\n",
       "      <td>...</td>\n",
       "      <td>-254.08</td>\n",
       "      <td>-266.79</td>\n",
       "      <td>-271.88</td>\n",
       "      <td>-264.24</td>\n",
       "      <td>-2483.807190</td>\n",
       "      <td>2.999879</td>\n",
       "      <td>-92.117900</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>-37.829751</td>\n",
       "      <td>196.964551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532152</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-71.63</td>\n",
       "      <td>-69.30</td>\n",
       "      <td>-57.67</td>\n",
       "      <td>-34.41</td>\n",
       "      <td>-39.06</td>\n",
       "      <td>-34.41</td>\n",
       "      <td>-59.99</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.75</td>\n",
       "      <td>-64.43</td>\n",
       "      <td>-64.43</td>\n",
       "      <td>-69.11</td>\n",
       "      <td>175.328754</td>\n",
       "      <td>-19.556319</td>\n",
       "      <td>12.784301</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>-37.829751</td>\n",
       "      <td>90.637259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85964</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51.26</td>\n",
       "      <td>11.85</td>\n",
       "      <td>11.85</td>\n",
       "      <td>7.19</td>\n",
       "      <td>7.19</td>\n",
       "      <td>21.18</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>13.42</td>\n",
       "      <td>21.52</td>\n",
       "      <td>183.210000</td>\n",
       "      <td>7.318705</td>\n",
       "      <td>2.136430</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>-37.829751</td>\n",
       "      <td>-35.444589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70965</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.78</td>\n",
       "      <td>74.15</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-21.03</td>\n",
       "      <td>-11.42</td>\n",
       "      <td>17.35</td>\n",
       "      <td>3.01</td>\n",
       "      <td>...</td>\n",
       "      <td>179.05</td>\n",
       "      <td>190.83</td>\n",
       "      <td>174.36</td>\n",
       "      <td>183.79</td>\n",
       "      <td>-265.227927</td>\n",
       "      <td>-12.081250</td>\n",
       "      <td>-2.321638</td>\n",
       "      <td>0.700258</td>\n",
       "      <td>-37.829751</td>\n",
       "      <td>-84.702777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724614</th>\n",
       "      <td>498</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.44</td>\n",
       "      <td>-82.44</td>\n",
       "      <td>-46.87</td>\n",
       "      <td>-46.87</td>\n",
       "      <td>-3140.908703</td>\n",
       "      <td>0.189787</td>\n",
       "      <td>-72.352847</td>\n",
       "      <td>1.875268</td>\n",
       "      <td>36.770159</td>\n",
       "      <td>-16.211651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502383</th>\n",
       "      <td>499</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.96</td>\n",
       "      <td>67.96</td>\n",
       "      <td>67.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.84</td>\n",
       "      <td>-33.84</td>\n",
       "      <td>-33.84</td>\n",
       "      <td>-33.84</td>\n",
       "      <td>579.798309</td>\n",
       "      <td>-2.075360</td>\n",
       "      <td>10.472275</td>\n",
       "      <td>1.875268</td>\n",
       "      <td>36.770159</td>\n",
       "      <td>-32.724634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520688</th>\n",
       "      <td>500</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3956.174713</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-107.902656</td>\n",
       "      <td>1.875268</td>\n",
       "      <td>36.770159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362717</th>\n",
       "      <td>501</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-269.470033</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.838784</td>\n",
       "      <td>1.875268</td>\n",
       "      <td>36.770159</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340701</th>\n",
       "      <td>502</td>\n",
       "      <td>1828</td>\n",
       "      <td>0</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-104.77</td>\n",
       "      <td>-543.588182</td>\n",
       "      <td>10.118735</td>\n",
       "      <td>-11.513152</td>\n",
       "      <td>1.875268</td>\n",
       "      <td>36.770159</td>\n",
       "      <td>-149.589372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        day  equity  rinitiale      r0      r1      r2      r3      r4  \\\n",
       "ID                                                                       \n",
       "832716    0       0          0    0.00    0.00  -12.61  -10.09   12.57   \n",
       "403174    1       0          0   34.83  -44.50  -26.97   -2.07    5.38   \n",
       "532152    2       0          0  -71.63  -69.30  -57.67  -34.41  -39.06   \n",
       "85964     3       0          0    0.00   51.26   11.85   11.85    7.19   \n",
       "70965     4       0          0   40.78   74.15    2.89  -21.03  -11.42   \n",
       "...     ...     ...        ...     ...     ...     ...     ...     ...   \n",
       "724614  498    1828          0    0.00    0.00    0.00    0.00    0.00   \n",
       "502383  499    1828          0    0.00    0.00    0.00    0.00   67.96   \n",
       "520688  500    1828          0    0.00    0.00    0.00    0.00    0.00   \n",
       "362717  501    1828          0    0.00    0.00    0.00    0.00    0.00   \n",
       "340701  502    1828          0 -104.77 -104.77 -104.77 -104.77 -104.77   \n",
       "\n",
       "            r5      r6  ...     r49     r50     r51     r52      auc_day  \\\n",
       "ID                      ...                                                \n",
       "832716   -2.50   -5.02  ...  138.81  136.33  156.20  158.68  1075.783995   \n",
       "403174    5.38   32.65  ... -254.08 -266.79 -271.88 -264.24 -2483.807190   \n",
       "532152  -34.41  -59.99  ...  -59.75  -64.43  -64.43  -69.11   175.328754   \n",
       "85964     7.19   21.18  ...   -0.49   -0.49   13.42   21.52   183.210000   \n",
       "70965    17.35    3.01  ...  179.05  190.83  174.36  183.79  -265.227927   \n",
       "...        ...     ...  ...     ...     ...     ...     ...          ...   \n",
       "724614    0.00    0.00  ...  -82.44  -82.44  -46.87  -46.87 -3140.908703   \n",
       "502383   67.96   67.96  ...  -33.84  -33.84  -33.84  -33.84   579.798309   \n",
       "520688    0.00    0.00  ...    0.00    0.00    0.00    0.00 -3956.174713   \n",
       "362717    0.00    0.00  ...    0.00    0.00    0.00    0.00  -269.470033   \n",
       "340701 -104.77 -104.77  ... -104.77 -104.77 -104.77 -104.77  -543.588182   \n",
       "\n",
       "             diff  r52_mean_day  r52_mean_equity  auc_equity  diff_equity  \n",
       "ID                                                                         \n",
       "832716   2.147903     49.866790         0.700258  -37.829751   -61.081025  \n",
       "403174   2.999879    -92.117900         0.700258  -37.829751   196.964551  \n",
       "532152 -19.556319     12.784301         0.700258  -37.829751    90.637259  \n",
       "85964    7.318705      2.136430         0.700258  -37.829751   -35.444589  \n",
       "70965  -12.081250     -2.321638         0.700258  -37.829751   -84.702777  \n",
       "...           ...           ...              ...         ...          ...  \n",
       "724614   0.189787    -72.352847         1.875268   36.770159   -16.211651  \n",
       "502383  -2.075360     10.472275         1.875268   36.770159   -32.724634  \n",
       "520688  -0.000000   -107.902656         1.875268   36.770159     0.000000  \n",
       "362717  -0.000000      1.838784         1.875268   36.770159     0.000000  \n",
       "340701  10.118735    -11.513152         1.875268   36.770159  -149.589372  \n",
       "\n",
       "[843299 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No groupby\n"
     ]
    }
   ],
   "source": [
    "cluster_type = \"returns\"\n",
    "\n",
    "df_train, df_test = preprocess_data(input_training, input_test, outlier=0.05, groupby=False, IQRoutlier = False, variables=\"market+volatility\", na_count=True, clusters = True, cluster_type = cluster_type, suppr = True, nbforlast = 18)\n",
    "\n",
    "# Définition de l'index des dataframes df_train et df_test\n",
    "df_train = df_train.set_index('ID')\n",
    "df_test = df_test.set_index('ID')\n",
    "\n",
    "def sharpe(df):\n",
    "    df['market_sharpe'] = df['market_mean_last_returns_day'] / df['market_mean_volatility_day']\n",
    "    df['sharpe'] = df['last_returns'] / df['volatility']\n",
    "\n",
    "sharpe_bool = False\n",
    "\n",
    "if sharpe_bool == 1:\n",
    "    sharpe(df_train)\n",
    "    sharpe(df_test)\n",
    "\n",
    "#Ajoute à df_train la colonne auc et diff de df_train_cumsum sur l'Index\n",
    "df_train = df_train.merge(df_train_cumsum[['auc_day', 'diff', 'r52_mean_day', 'r52_mean_equity', 'r52', 'auc_equity', 'diff_equity']], on = 'ID', how='left')\n",
    "df_test = df_test.merge(df_test_cumsum[['auc_day', 'diff', 'r52_mean_day', 'r52_mean_equity', 'r52', 'auc_equity', 'diff_equity']], on = 'ID', how='left')\n",
    "\n",
    "# Ajout de 1 à la colonne 'reod' de df_train\n",
    "df_train['reod'] = df_train['reod'] + 1\n",
    "\n",
    "# Séparation des données d'entraînement et de test\n",
    "train = df_train[df_train['day'] <= 350]\n",
    "test = df_train[df_train['day'] >= 351]\n",
    "\n",
    "if cluster_type == \"returns\":\n",
    "    suppr = ['day', 'equity', 'r52_mean_equity', 'auc_equity','equity_volatility_beta', 'diff','volatility', 'diff_equity' ,'r52','returns','last_returns','market_mean_last_returns_day','day_last_returns_beta']\n",
    "\n",
    "elif cluster_type == \"volatility\":\n",
    "    suppr = ['day', 'equity','volatility', 'returns']\n",
    "    \n",
    "df_test = df_test.drop(suppr, axis = 1)\n",
    "df_train = df_train.drop(suppr, axis = 1)\n",
    "#Drop suppr\n",
    "test = test.drop(suppr, axis = 1)\n",
    "train = train.drop(suppr, axis = 1)\n",
    "\n",
    "\n",
    "# Séparation des features (X) et de la variable cible (y) pour l'entraînement et le test\n",
    "X_train = train.drop('reod', axis=1)\n",
    "y_train = train['reod']\n",
    "X_test = test.drop('reod', axis=1)\n",
    "y_test = test['reod']\n",
    "\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "dall_clf = xgb.DMatrix(df_train.drop('reod', axis=1), df_train['reod'], enable_categorical=True)\n",
    "dpred_clf = xgb.DMatrix(df_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mlogloss:1.05759\ttrain-mlogloss:1.05944\n",
      "[1]\tvalidation-mlogloss:1.03364\ttrain-mlogloss:1.03722\n",
      "[2]\tvalidation-mlogloss:1.01878\ttrain-mlogloss:1.02219\n",
      "RMSE of the base model: 0.967\n",
      "Accuracy of the base model: 0.491\n",
      "Ratio de 1 bien prédit: 0.1358510728044768\n",
      "Ratio de 0 bien prédit: 0.7267016591705087\n",
      "Ratio de -1 bien prédit: 0.4924973482006571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#Modèle qui s'entraine sur toute la base\n",
    "params = {\"objective\": \"multi:softmax\",\"num_class\": 3, \"tree_method\": \"hist\", \"random_state\": 0}\n",
    "          \n",
    "\n",
    "params = {\"objective\": \"multi:softmax\",\"num_class\": 3, \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.3, \"max_depth\": 6,\n",
    "            \"gamma\": 0, \"subsample\": 1, \"colsample_bytree\": 1,\n",
    "            \"alpha\": 0, \"lambda\": 1,\"random_state\": 0}\n",
    "\n",
    "n = 3\n",
    "\n",
    "\n",
    "evals = [(dtest_clf, \"validation\"), (dtrain_clf, \"train\")]\n",
    "\n",
    "\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dall_clf,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval=1,\n",
    "   # Activate early stopping\n",
    "   early_stopping_rounds=30\n",
    ")\n",
    "preds = model.predict(dtest_clf)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "accuracy = accuracy_score(y_test, preds.round())\n",
    "\n",
    "print(f\"RMSE of the base model: {rmse:.3f}\")\n",
    "print(f\"Accuracy of the base model: {accuracy:.3f}\") \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, preds)\n",
    "\n",
    "# Calculer le ratio de 1, 0 et -1 correctement prédits\n",
    "ratio_1 = conf_matrix[2, 2] / conf_matrix[2].sum()\n",
    "ratio_0 = conf_matrix[1, 1] / conf_matrix[1].sum()\n",
    "ratio_minus_1 = conf_matrix[0, 0] / conf_matrix[0].sum()\n",
    "\n",
    "print(f\"Ratio de 1 bien prédit: {ratio_1}\")\n",
    "print(f\"Ratio de 0 bien prédit: {ratio_0}\")\n",
    "print(f\"Ratio de -1 bien prédit: {ratio_minus_1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
