{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"/Users/julesmourgues/Documents/Programmation/CFM/\"\n",
    "\n",
    "#Load x and y train data\n",
    "input_test = pd.read_csv(path + \"input_test.csv\")\n",
    "input_training = pd.read_csv(path + \"input_training.csv\")\n",
    "#output_test_random = pd.read_csv(path + \"output_test_random.csv\")\n",
    "output_training = pd.read_csv(path + \"output_training_gmEd6Zt.csv\")\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def create_and_assign_clusters(df, num_clusters, column):\n",
    "    # Sort by equity and day\n",
    "    if column == \"returns\":\n",
    "        df['func'] = df.iloc[:, 3:3+53].sum(axis = 1) # Sum of the 52 first columns\n",
    "    elif column == \"volatility\":\n",
    "        df['func'] = df.iloc[:, 3:3+53].std(axis = 1)\n",
    "\n",
    "    new_df = df.pivot_table(index='day', columns='equity', values='func')\n",
    "    new_df.columns.name = None\n",
    "    func = new_df.fillna(0)\n",
    "\n",
    "    # Calculate the Pearson correlation matrix and convert it to distances\n",
    "    distance_matrix = np.sqrt(0.5 * (1 - func.corr(method='pearson')))\n",
    "\n",
    "    # Perform hierarchical clustering using Ward's method\n",
    "    linked = linkage(np.nan_to_num(squareform(distance_matrix, checks=False), nan=0, posinf=1e9, neginf=-1e9), method='ward')\n",
    "\n",
    "    # Use the fcluster function to assign cluster labels to tickers\n",
    "    cluster_labels = fcluster(linked, num_clusters, criterion='maxclust')\n",
    "\n",
    "    # Create a DataFrame with the cluster labels and corresponding tickers\n",
    "    clustered_data = pd.DataFrame({'Ticker': func.columns, 'Cluster': cluster_labels})\n",
    "\n",
    "    # Use a loop to extract each cluster and store it in a separate DataFrame\n",
    "    clusters = {i: clustered_data[clustered_data['Cluster'] == i]['Ticker'] for i in range(1, num_clusters + 1)}\n",
    "\n",
    "    if df['equity'].max() >= 1000000:\n",
    "        for cluster in clusters:\n",
    "            for index in clusters[cluster].index:\n",
    "                if column == \"returns\":\n",
    "                    df.loc[df['equity'] == (1000000 + index), 'cluster_returns'] = cluster\n",
    "                elif column == \"volatility\":\n",
    "                    df.loc[df['equity'] == (1000000 + index), 'cluster_volatility'] = cluster\n",
    "    else:\n",
    "        for cluster in clusters:\n",
    "            for index in clusters[cluster].index:\n",
    "                if column == \"returns\":\n",
    "                    df.loc[df['equity'] == index, 'cluster_returns'] = cluster\n",
    "                elif column == \"volatility\":\n",
    "                    df.loc[df['equity'] == index, 'cluster_volatility'] = cluster\n",
    "\n",
    "    #Suoprimer la colonne returns\n",
    "    df = df.drop(columns = ['func'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(input_training, input_test, outlier=0.05, groupby=False, IQRoutlier = True, variables = False, na_count = False, clusters = False, cluster_type = \"returns\", suppr = False, nbforlast = 5):\n",
    "    import pandas as pd\n",
    "    from datetime import time, timedelta, datetime, date\n",
    "\n",
    "    df_train = input_training.copy()\n",
    "    #Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "    if na_count is not False:\n",
    "        df_train['na_count'] = df_train.isnull().sum(axis=1)\n",
    "      \n",
    "        #Remplace les inf par des 60\n",
    "        df_train = df_train.replace([np.inf, -np.inf], 60)\n",
    "    df_train = df_train.fillna(0)\n",
    "    df_train = df_train.sort_values(by=['equity', 'day'])\n",
    "\n",
    "    df_test = input_test.copy()\n",
    "    #Ajoute ue colonne qui compte le nombre de na dans chaque ligne\n",
    "    if na_count is not False:\n",
    "        df_test['na_count'] = df_test.isnull().sum(axis=1)\n",
    "\n",
    "        #Remplace les inf par des 60\n",
    "        df_test = df_test.replace([np.inf, -np.inf], 60)\n",
    "    df_test = df_test.fillna(0)\n",
    "    df_test = df_test.sort_values(by=['equity', 'day'])\n",
    "\n",
    "    if outlier is not False:\n",
    "        for col in df_train.iloc[:,3:3+53].columns:\n",
    "            Q1 = df_train[col].quantile(outlier)\n",
    "            Q3 = df_train[col].quantile(1 - outlier)\n",
    "            IQR = Q3 - Q1\n",
    "            median = df_train[col].median()\n",
    "            if IQRoutlier is not False:\n",
    "                df_train.loc[(df_train[col] < (Q1 - 1.5 * IQR)), col] = Q1 - 1.5 * IQR\n",
    "                df_train.loc[(df_train[col] > (Q3 + 1.5 * IQR)), col] = Q3 + 1.5 * IQR\n",
    "            else:\n",
    "                df_train.loc[((df_train[col] < ((Q1 - 1.5 * IQR))) | (df_train[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "        for col in df_test.iloc[:,3:3+53].columns:\n",
    "            Q1 = df_test[col].quantile(outlier)\n",
    "            Q3 = df_test[col].quantile(1 - outlier)\n",
    "            IQR = Q3 - Q1\n",
    "            median = df_test[col].median()\n",
    "            if IQRoutlier is not False:\n",
    "                df_test.loc[(df_test[col] < (Q1 - 1.5 * IQR)), col] = Q1 - 1.5 * IQR\n",
    "                df_test.loc[(df_test[col] > (Q3 + 1.5 * IQR)), col] = Q3 + 1.5 * IQR\n",
    "            else:\n",
    "                df_test.loc[((df_test[col] < ((Q1 - 1.5 * IQR))) | (df_test[col] > (Q3 + 1.5 * IQR))), col] = median\n",
    "\n",
    "\n",
    "    if clusters is not False:\n",
    "        if cluster_type == \"returns\":\n",
    "            df_train = create_and_assign_clusters(df_train, clusters, \"returns\")\n",
    "            df_test = create_and_assign_clusters(df_test, clusters, \"returns\")\n",
    "        elif cluster_type == \"volatility\":\n",
    "            df_train = create_and_assign_clusters(df_train, clusters, \"volatility\")\n",
    "            df_test = create_and_assign_clusters(df_test, clusters, \"volatility\")\n",
    "\n",
    "    def volatility(df):\n",
    "            df['volatility'] = df.iloc[:,3:3+53].std(axis=1)\n",
    "            df['returns'] = df.iloc[:,3:3+53].sum(axis=1)\n",
    "            df['last_returns'] = df.iloc[:,3+53-nbforlast:3+53].iloc[:,-1]\n",
    "\n",
    "    if variables == \"volatility\":\n",
    "        volatility(df_train)\n",
    "        volatility(df_test)\n",
    "    elif variables == \"market+volatility\":\n",
    "        volatility(df_train)\n",
    "        volatility(df_test)\n",
    "\n",
    "        def calculate_market_means1(df, group_col, value_cols, beta = True):\n",
    "\n",
    "            grouped_values = df.groupby(group_col)[value_cols]\n",
    "\n",
    "            mean_col = grouped_values.mean()\n",
    "            #rolling_mean_col = mean_col.rolling(5).mean()\n",
    "\n",
    "            df[f'market_mean_{value_cols}_{group_col}'] = df[group_col].map(mean_col)\n",
    "            if beta == True:\n",
    "                #df[f'rolling_market_mean_{value_col}_{group_col}'] = df[group_col].map(rolling_mean_col)\n",
    "                df[f'{group_col}_{value_cols}_beta'] = df[value_cols] / df[f'market_mean_{value_cols}_{group_col}']\n",
    "\n",
    "            '''if IQRindicateur is True:\n",
    "                if value_col == 'volatility':\n",
    "                    Q1 = grouped_values.quantile(0.25)\n",
    "                    Q3 = grouped_values.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "\n",
    "                    df[f'{value_col}_Q1_{group_col}'] = df[group_col].map(Q1)\n",
    "                    df[f'{value_col}_Q3_{group_col}'] = df[group_col].map(Q3)\n",
    "                    df[f'{value_col}_IQR_{group_col}'] = df[group_col].map(IQR)'''\n",
    "\n",
    "        def calculate_market_means(df, group_cols, value_cols, beta = True):\n",
    "            grouped_values = df.groupby(group_cols)[value_cols]\n",
    "\n",
    "            mean_col = grouped_values.mean()\n",
    "\n",
    "            # Create a multi-indexed series with the mean values\n",
    "            mean_series = pd.Series(mean_col.values, index=pd.MultiIndex.from_tuples(mean_col.index))\n",
    "\n",
    "            # Map the multi-indexed series to the original dataframe\n",
    "            df[f'market_mean_{value_cols}_{\"_\".join(group_cols)}'] = df.set_index(group_cols).index.map(mean_series)\n",
    "\n",
    "            if beta == True:\n",
    "                df[f'{\"_\".join(group_cols)}_{value_cols}_beta'] = df[value_cols] / df[f'market_mean_{value_cols}_{\"_\".join(group_cols)}']\n",
    "\n",
    "\n",
    "\n",
    "        # Utiliser la fonction calculate_market_means pour les colonnes 'returns' et 'volatility'\n",
    "        calculate_market_means1(df_train, 'day', 'volatility') \n",
    "        calculate_market_means1(df_test, 'day', 'volatility')\n",
    "        calculate_market_means1(df_train, 'equity', 'volatility') \n",
    "        calculate_market_means1(df_test, 'equity', 'volatility')\n",
    "        #calculate_market_means1(df_train, 'day', 'returns')\n",
    "        #calculate_market_means1(df_test, 'day', 'returns')\n",
    "        calculate_market_means1(df_train, 'day', 'last_returns')\n",
    "        calculate_market_means1(df_test, 'day', 'last_returns')\n",
    "\n",
    "        if cluster_type == \"returns\" and clusters is not False:\n",
    "            calculate_market_means(df_train, ['cluster_returns', 'day'], 'volatility', beta = True)\n",
    "            calculate_market_means(df_test, ['cluster_returns', 'day'], 'volatility', beta = True)\n",
    "        elif cluster_type == \"volatility\" and clusters is not False:\n",
    "            calculate_market_means(df_train, ['cluster_volatility', 'day'], 'volatility', beta = True)\n",
    "            calculate_market_means(df_test, ['cluster_volatility', 'day'], 'volatility', beta = True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        #calculate_market_means(df_test, 'day', 'volatility') \n",
    "    #calculate_market_means(df, 'equity', ['returns', 'volatility', 'last_returns', 'last_volatility']) \n",
    "\n",
    "    if groupby is False:\n",
    "        #df_train['next_day_r0'] = df_train.groupby('equity')['r0'].shift(-1)\n",
    "        #df_test['next_day_r0'] = df_test.groupby('equity')['r0'].shift(-1)\n",
    "        print(\"No groupby\")\n",
    "    else:\n",
    "        firstrtime = 35\n",
    "        new_labels = []\n",
    "        start_time = time(9, firstrtime)\n",
    "        for i in range(53):\n",
    "            new_labels.append(start_time.strftime('%H:%M'))\n",
    "            start_time = (datetime.combine(date.today(), start_time) + timedelta(minutes=5)).time()\n",
    "\n",
    "        col_rename_dict = {f'r{i}': new_labels[i] for i in range(53)}\n",
    "        df_train.rename(columns=col_rename_dict, inplace=True)\n",
    "        df_test.rename(columns=col_rename_dict, inplace=True)\n",
    "\n",
    "        id_column_train = df_train['ID']\n",
    "        id_column_test = df_test['ID']\n",
    "        interval_dict = {label: \"{:.2f}\".format(datetime.strptime(label, '%H:%M').hour + (datetime.strptime(label, '%H:%M').minute // groupby) / (60/groupby)) for label in new_labels}\n",
    "        \n",
    "        df_train_grouped_interval = df_train.groupby(interval_dict, axis=1).sum()\n",
    "        df_train_grouped_interval['ID'] = id_column_train\n",
    "        df_train = df_train.merge(df_train_grouped_interval, on = 'ID', how='left')\n",
    "        df_train = df_train.drop(df_train.columns[3:3+53], axis = 1)\n",
    "        #df_train['next_day_first_interval'] = df_train.groupby('equity')[list(interval_dict.values())[1]].shift(-1)\n",
    "\n",
    "        df_test_grouped_interval = df_test.groupby(interval_dict, axis=1).sum()\n",
    "        df_test_grouped_interval['ID'] = id_column_test\n",
    "        df_test = df_test.merge(df_test_grouped_interval, on = 'ID', how='left')\n",
    "        df_test = df_test.drop(df_test.columns[3:3+53], axis = 1)\n",
    "        #df_test['next_day_first_interval'] = df_test.groupby('equity')[list(interval_dict.values())[1]].shift(-1)\n",
    "\n",
    "    if suppr is True:\n",
    "        df_train = df_train.drop(df_train.columns[3:3+53], axis = 1)\n",
    "        df_test = df_test.drop(df_test.columns[3:3+53], axis = 1)\n",
    "\n",
    "    df_train = df_train.merge(output_training, on = 'ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentative de Voting Class en utilisant des data basés sur les clusters rendements pour le modèle 1, et les cluster de volatilités pour le modèle 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle 2 : Preprocess et modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No groupby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.05413\n",
      "[1]\tvalidation_0-mlogloss:1.02933\n",
      "[2]\tvalidation_0-mlogloss:1.01409\n",
      "RMSE du modèle de base : 0.963\n",
      "Accuracy du modèle de base : 0.491\n",
      "Ratio de 1 bien prédit : 0.2060262647282135\n",
      "Ratio de 0 bien prédit : 0.7348989192386783\n",
      "Ratio de -1 bien prédit : 0.4180658681085556\n"
     ]
    }
   ],
   "source": [
    "cluster_type = \"returns\"\n",
    "\n",
    "df_train, df_test = preprocess_data(input_training, input_test, outlier=0.05, groupby=False, IQRoutlier = False, variables=\"market+volatility\", na_count=True, clusters = 12, cluster_type = cluster_type)\n",
    "\n",
    "# Définition de l'index des dataframes df_train et df_test\n",
    "df_train = df_train.set_index('ID')\n",
    "df_test = df_test.set_index('ID')\n",
    "\n",
    "# Ajout de 1 à la colonne 'reod' de df_train\n",
    "df_train['reod'] = df_train['reod'] + 1\n",
    "\n",
    "# Séparation des données d'entraînement et de test\n",
    "train = df_train[df_train['day'] <= 350]\n",
    "test = df_train[df_train['day'] >= 351]\n",
    "\n",
    "if cluster_type == \"returns\":\n",
    "    suppr = ['day', 'equity','volatility', 'cluster_returns','returns']\n",
    "elif cluster_type == \"volatility\":\n",
    "    suppr = ['day', 'equity','volatility', 'cluster_volatility','returns']\n",
    "    \n",
    "df_test = df_test.drop(suppr, axis = 1)\n",
    "df_train = df_train.drop(suppr, axis = 1)\n",
    "#Drop suppr\n",
    "test = test.drop(suppr, axis = 1)\n",
    "train = train.drop(suppr, axis = 1)\n",
    "\n",
    "\n",
    "# Séparation des features (X) et de la variable cible (y) pour l'entraînement et le test\n",
    "X_train = train.drop('reod', axis=1)\n",
    "y_train = train['reod']\n",
    "X_test = test.drop('reod', axis=1)\n",
    "y_test = test['reod']\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Prédiction sur les données de test\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Modèle qui s'entraine sur toute la base\n",
    "params = {\"objective\": \"multi:softmax\",\"num_class\": 3, \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.3, \"max_depth\": 6,\n",
    "            \"gamma\": 0, \"subsample\": 1, \"colsample_bytree\": 1,\n",
    "            \"alpha\": 0, \"lambda\": 1,\"random_state\": 0}\n",
    "n = 3\n",
    "\n",
    "\n",
    "#Create an XGBClassifier\n",
    "model_XG1 = XGBClassifier(n_estimators=n, **params)\n",
    "\n",
    "# Train the model\n",
    "model_XG1.fit(df_train.drop('reod', axis=1), df_train['reod'], eval_set=[(X_test, y_test)], early_stopping_rounds=30, verbose=1)\n",
    "\n",
    "preds_XG = model_XG1.predict(X_test)\n",
    "preds_XG1 = model_XG1.predict_proba(df_train.drop('reod', axis=1))\n",
    "\n",
    "# Calcul du RMSE et de l'accuracy\n",
    "rmse = mean_squared_error(y_test, preds_XG, squared=False)\n",
    "accuracy = accuracy_score(y_test, preds_XG.round())\n",
    "\n",
    "print(f\"RMSE du modèle de base : {rmse:.3f}\")\n",
    "print(f\"Accuracy du modèle de base : {accuracy:.3f}\") \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calcul de la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, preds_XG)\n",
    "\n",
    "# Calcul du ratio de 1, 0 et -1 correctement prédits\n",
    "ratio_1 = conf_matrix[2, 2] / conf_matrix[2].sum()\n",
    "ratio_0 = conf_matrix[1, 1] / conf_matrix[1].sum()\n",
    "ratio_minus_1 = conf_matrix[0, 0] / conf_matrix[0].sum()\n",
    "\n",
    "print(f\"Ratio de 1 bien prédit : {ratio_1}\")\n",
    "print(f\"Ratio de 0 bien prédit : {ratio_0}\")\n",
    "print(f\"Ratio de -1 bien prédit : {ratio_minus_1}\")\n",
    "\n",
    "probs_XG1 = model_XG1.predict_proba(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle 2 : Preprocess et modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No groupby\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.05412\n",
      "[1]\tvalidation_0-mlogloss:1.02931\n",
      "[2]\tvalidation_0-mlogloss:1.01401\n",
      "RMSE du modèle de base : 0.960\n",
      "Accuracy du modèle de base : 0.491\n",
      "Ratio de 1 bien prédit : 0.20021934141334607\n",
      "Ratio de 0 bien prédit : 0.739321495976488\n",
      "Ratio de -1 bien prédit : 0.41684992109280006\n"
     ]
    }
   ],
   "source": [
    "cluster_type = \"volatility\"\n",
    "\n",
    "df_train, df_test = preprocess_data(input_training, input_test, outlier=0.05, groupby=False, IQRoutlier = False, variables=\"market+volatility\", na_count=True, clusters = 15, cluster_type = cluster_type)\n",
    "\n",
    "# Définition de l'index des dataframes df_train et df_test\n",
    "df_train = df_train.set_index('ID')\n",
    "df_test = df_test.set_index('ID')\n",
    "\n",
    "# Ajout de 1 à la colonne 'reod' de df_train\n",
    "df_train['reod'] = df_train['reod'] + 1\n",
    "\n",
    "# Séparation des données d'entraînement et de test\n",
    "train = df_train[df_train['day'] <= 350]\n",
    "test = df_train[df_train['day'] >= 351]\n",
    "\n",
    "if cluster_type == \"returns\":\n",
    "    suppr = ['day', 'equity','volatility', 'cluster_returns','returns']\n",
    "elif cluster_type == \"volatility\":\n",
    "    suppr = ['day', 'equity','volatility', 'cluster_volatility','returns']\n",
    "    \n",
    "df_test = df_test.drop(suppr, axis = 1)\n",
    "df_train = df_train.drop(suppr, axis = 1)\n",
    "#Drop suppr\n",
    "test = test.drop(suppr, axis = 1)\n",
    "train = train.drop(suppr, axis = 1)\n",
    "\n",
    "\n",
    "# Séparation des features (X) et de la variable cible (y) pour l'entraînement et le test\n",
    "X_train = train.drop('reod', axis=1)\n",
    "y_train = train['reod']\n",
    "X_test = test.drop('reod', axis=1)\n",
    "y_test = test['reod']\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Prédiction sur les données de test\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Modèle qui s'entraine sur toute la base\n",
    "params = {\"objective\": \"multi:softmax\",\"num_class\": 3, \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.3, \"max_depth\": 6,\n",
    "            \"gamma\": 0, \"subsample\": 1, \"colsample_bytree\": 1,\n",
    "            \"alpha\": 0, \"lambda\": 1,\"random_state\": 0}\n",
    "n = 3\n",
    "\n",
    "\n",
    "#Create an XGBClassifier\n",
    "model_XG2 = XGBClassifier(n_estimators=n, **params)\n",
    "\n",
    "# Train the model\n",
    "model_XG2.fit(df_train.drop('reod', axis=1), df_train['reod'], eval_set=[(X_test, y_test)], early_stopping_rounds=30, verbose=1)\n",
    "\n",
    "preds_XG = model_XG2.predict(X_test)\n",
    "preds_XG2 = model_XG2.predict_proba(df_train.drop('reod', axis=1))\n",
    "# Calcul du RMSE et de l'accuracy\n",
    "rmse = mean_squared_error(y_test, preds_XG, squared=False)\n",
    "accuracy = accuracy_score(y_test, preds_XG.round())\n",
    "\n",
    "print(f\"RMSE du modèle de base : {rmse:.3f}\")\n",
    "print(f\"Accuracy du modèle de base : {accuracy:.3f}\") \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calcul de la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, preds_XG)\n",
    "\n",
    "# Calcul du ratio de 1, 0 et -1 correctement prédits\n",
    "ratio_1 = conf_matrix[2, 2] / conf_matrix[2].sum()\n",
    "ratio_0 = conf_matrix[1, 1] / conf_matrix[1].sum()\n",
    "ratio_minus_1 = conf_matrix[0, 0] / conf_matrix[0].sum()\n",
    "\n",
    "print(f\"Ratio de 1 bien prédit : {ratio_1}\")\n",
    "print(f\"Ratio de 0 bien prédit : {ratio_0}\")\n",
    "print(f\"Ratio de -1 bien prédit : {ratio_minus_1}\")\n",
    "\n",
    "probs_XG2 = model_XG2.predict_proba(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait le voting class des deux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE du modèle de vote : 1.022\n",
      "Accuracy du modèle de vote : 0.467\n",
      "Ratio de 1 bien prédit : 0.20412811788194932\n",
      "Ratio de 0 bien prédit : 0.6943633273551865\n",
      "Ratio de -1 bien prédit : 0.3964245983494166\n"
     ]
    }
   ],
   "source": [
    "# Importer la classe VotingClassifier du module sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Créer une liste de classifieurs pour le vote\n",
    "classifiers = [('xgb1', model_XG1), ('xgb2', model_XG2)]\n",
    "\n",
    "# Créer un VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# Entraîner le VotingClassifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les résultats sur l'ensemble de test\n",
    "voting_preds = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculer le RMSE et l'accuracy\n",
    "rmse = mean_squared_error(y_test, voting_preds, squared=False)\n",
    "accuracy = accuracy_score(y_test, voting_preds.round())\n",
    "\n",
    "print(f\"RMSE du modèle de vote : {rmse:.3f}\")\n",
    "print(f\"Accuracy du modèle de vote : {accuracy:.3f}\") \n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, voting_preds)\n",
    "\n",
    "# Calculer le ratio de 1, 0 et -1 correctement prédits\n",
    "ratio_1 = conf_matrix[2, 2] / conf_matrix[2].sum()\n",
    "ratio_0 = conf_matrix[1, 1] / conf_matrix[1].sum()\n",
    "ratio_minus_1 = conf_matrix[0, 0] / conf_matrix[0].sum()\n",
    "\n",
    "print(f\"Ratio de 1 bien prédit : {ratio_1}\")\n",
    "print(f\"Ratio de 0 bien prédit : {ratio_0}\")\n",
    "print(f\"Ratio de -1 bien prédit : {ratio_minus_1}\")\n",
    "\n",
    "# Obtenir les prédictions brutes\n",
    "voting_results = voting_clf.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moyenne des scores puis Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Moyenne des probabilités\n",
    "combined_probs = (probs_XG1 + probs_XG2) / 2\n",
    "\n",
    "# Choisir les classes avec la probabilité la plus élevée\n",
    "combined_preds = np.argmax(combined_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prendre le maximum des probabilités des deux modèles\n",
    "max_probs = np.maximum(probs_XG1, probs_XG2)\n",
    "\n",
    "# Choisir la classe avec la probabilité la plus élevée\n",
    "max_voting_predictions = np.argmax(max_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Supposons que model_XG1 et model_XG2 sont déjà entraînés\n",
    "#df_train.drop('reod', axis=1), df_train['reod']\n",
    "# Prédictions des probabilités sur l'ensemble de validation\n",
    "\n",
    "# Empiler les prédictions pour former de nouvelles features pour le métamodèle\n",
    "meta_features = np.column_stack([preds_XG1, preds_XG2])\n",
    "\n",
    "# Entraîner le métamodèle\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(meta_features, df_train['reod'])\n",
    "\n",
    "# Utiliser le métamodèle pour faire des prédictions finales\n",
    "# Prédictions des probabilités sur l'ensemble de tes\n",
    "\n",
    "# Empiler les prédictions de test\n",
    "test_meta_features = np.column_stack([probs_XG1, probs_XG2])\n",
    "\n",
    "# Prédictions finales\n",
    "final_predictions = meta_model.predict(test_meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pred and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_voting_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prédire les valeurs de test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmax_voting_predictions\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Crée un dataframe avec les résultats\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreod\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_voting_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Prédire les valeurs de test\n",
    "results = max_voting_predictions\n",
    "\n",
    "#Crée un dataframe avec les résultats\n",
    "results = pd.DataFrame(results, columns = ['reod'])\n",
    "#Ajoute une colonne ID\n",
    "results['ID'] = df_test.index\n",
    "#Reorder columns\n",
    "results = results[['ID', 'reod']]\n",
    "#Remplace reod = 2 par reod = -1\n",
    "results['reod'] = results['reod'] - 1\n",
    "#export to csv\n",
    "results.to_csv(path+'Combined maxvoting 10 10.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# Save the model\n",
    "dump(model, path+'model_Xgboost_lr0,06RM7_train_seed0_n55_350day.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
